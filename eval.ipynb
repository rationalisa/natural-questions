{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom datasets import load_dataset\\ndataset_name = \\'squad\\'\\ndatasets = load_dataset(dataset_name)\\ntrain_examples = datasets[\\'train\\']\\ntrain_dataset = train_examples.map(prepare_validation_features, batched=True, remove_columns=train_examples.column_names)\\neval_examples = datasets[\\'validation\\'].select(range(max_val_samples))\\neval_dataset = eval_examples.map(prepare_validation_features, batched=True, remove_columns=eval_examples.column_names)\\n\\n#train_dataset=load_from_disk(\"/storage/{}/train_{}_{}\".format(dir_name, 20000,15950)).shuffle()\\n\\neval_examples = load_from_disk(\"/storage/{}/val_example\".format(dir_name))\\neval_examples = eval_examples.select(range(max_val_samples))\\neval_dataset = eval_examples.map(prepare_validation_features, batched=True, remove_columns=eval_examples.column_names)\\neval_dataset = eval_dataset.select(range(max_val_samples))\\n#eval_dataset = load_from_disk(\"/storage/{}/val\".format(dir_name))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import concatenate_datasets\n",
    "from datasets import load_from_disk, load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from utils_qa import postprocess_qa_predictions\n",
    "from trainer_qa import QuestionAnsweringTrainer\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer,EvalPrediction\n",
    "from transformers import default_data_collator\n",
    "\n",
    "max_length = 512\n",
    "doc_stride = 128\n",
    "batch_size = 4\n",
    "\n",
    "model_pretrain = \"bert-large-cased-whole-word-masking-finetuned-squad\" #'roberta-large' \n",
    "dir_name = 'BERT_SQUAD'#'roberta'\n",
    "cp = 'checkpoint-5000'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"/storage/model/{}/{}\".format(dir_name,cp))\n",
    "\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(model_pretrain)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_pretrain,\n",
    "    use_fast=True,\n",
    ")\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "'''\n",
    "from datasets import load_dataset\n",
    "dataset_name = 'squad'\n",
    "datasets = load_dataset(dataset_name)\n",
    "train_examples = datasets['train']\n",
    "train_dataset = train_examples.map(prepare_validation_features, batched=True, remove_columns=train_examples.column_names)\n",
    "eval_examples = datasets['validation'].select(range(max_val_samples))\n",
    "eval_dataset = eval_examples.map(prepare_validation_features, batched=True, remove_columns=eval_examples.column_names)\n",
    "\n",
    "#train_dataset=load_from_disk(\"/storage/{}/train_{}_{}\".format(dir_name, 20000,15950)).shuffle()\n",
    "\n",
    "eval_examples = load_from_disk(\"/storage/{}/val_example\".format(dir_name))\n",
    "eval_examples = eval_examples.select(range(max_val_samples))\n",
    "eval_dataset = eval_examples.map(prepare_validation_features, batched=True, remove_columns=eval_examples.column_names)\n",
    "eval_dataset = eval_dataset.select(range(max_val_samples))\n",
    "#eval_dataset = load_from_disk(\"/storage/{}/val\".format(dir_name))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_data import read_annotation_gzip\n",
    "from datasets import Dataset\n",
    "max_val_samples= 200\n",
    "path = '/storage/datset/v1.0_sample_nq-dev-sample.jsonl.gz'\n",
    "dic = read_annotation_gzip(path)\n",
    "eval_examples = Dataset.from_dict(dic).select(range(max_val_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2894c2dc124db0a56549b748a6d979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8660\n",
      "attention_mask\n",
      "end_positions\n",
      "example_id\n",
      "input_ids\n",
      "offset_mapping\n",
      "start_positions\n",
      "token_type_ids\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = eval_examples.map(prepare_validation_features, batched=True, remove_columns=eval_examples.column_names)\n",
    "print(len(eval_dataset))\n",
    "for key in eval_dataset[0]:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 10:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996b39f059884c719c811dad068a0af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving predictions to /storage/model/BERT_SQUAD/eval_predictions.json.\n",
      "Saving nbest_preds to /storage/model/BERT_SQUAD/eval_nbest_predictions.json.\n",
      "{'eval_loss': 0.15150883793830872}\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"squad_v2\")\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"/storage/model/{}\".format(dir_name),\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    save_steps = 600,\n",
    "    eval_steps = 600,\n",
    "    evaluation_strategy ='steps',\n",
    "    gradient_accumulation_steps=8,\n",
    ")\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=None,\n",
    "    eval_dataset=eval_dataset,\n",
    "    eval_examples=eval_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    post_process_function=post_processing_function,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "#metrics[\"eval_samples\"] = min(max_val_samples, len(eval_dataset))\n",
    "#trainer.log_metrics(\"eval\", metrics)\n",
    "#trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38271820545196533}\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation preprocessing\n",
    "def prepare_validation_features(examples):\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    #examples = simplify_nq_example(examples)\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "    # corresponding example_id and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)           \n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "        ## TODO for gz file\n",
    "        start_char, end_char = -1, -1\n",
    "        # Start/end character index of the answer in the text.\n",
    "        for candidate in examples[\"annotations\"][sample_index]:\n",
    "            cur = candidate[\"long_answer\"][\"start_char\"]\n",
    "            if cur != -1:\n",
    "                start_char = cur\n",
    "                end_char = candidate[\"long_answer\"][\"end_char\"]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if start_char == -1:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "                \n",
    "    return tokenized_examples\n",
    "\n",
    "# Training preprocessing\n",
    "def prepare_train_features(examples):\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        ## TODO for gz file\n",
    "        start_char, end_char = -1, -1\n",
    "        # Start/end character index of the answer in the text.\n",
    "        for candidate in examples[\"annotations\"][sample_index]:\n",
    "            cur = candidate[\"long_answer\"][\"start_char\"]\n",
    "            if cur != -1:\n",
    "                start_char = cur\n",
    "                end_char = candidate[\"long_answer\"][\"end_char\"]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if start_char == -1:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "from utils_qa import postprocess_qa_predictions\n",
    "# Post-processing:\n",
    "def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
    "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "    predictions = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        n_best_size=20,\n",
    "        output_dir= args.output_dir,\n",
    "        is_world_process_zero=trainer.is_world_process_zero(),\n",
    "        prefix=stage,\n",
    "        version_2_with_negative=True,\n",
    "    )\n",
    "    # Format the result to the format the metric expects.\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "    #references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "\n",
    "    references = [{\"id\": ex['id'], 'start_token': ex['annotations'][0]['long_answer']['start_token'], \n",
    "                   'end_token': ex['annotations'][0]['long_answer']['end_token']\n",
    "      } for ex in examples ]\n",
    "\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
